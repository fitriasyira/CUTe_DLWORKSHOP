{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fitriasyira/CUTe_DLWORKSHOP/blob/main/Assessment1_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpRNh1-L8zuk"
      },
      "source": [
        "## Assessment 1: Deep Learning\n",
        "\n",
        "1) Answer all questions.\n",
        "2) This assessment is open-book. You are allowed to refer to any references including online materials, books, notes, codes, github links, etc.\n",
        "3) Copy this notebook to your google drive (click **FILE** > **save a copy in Drive**)\n",
        "4) Upload the answer notebook to your github. \n",
        "5) Submit the assessment by sharing the link to your answer notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRauIpz8zun"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**QUESTION 1** \n",
        "\n",
        "One day while wandering around a clothing store at KL East Mall, you stumbled upon a pretty girl who is choosing a dress for Hari Raya. It turns out that the girl is visually impaired and had a hard time distinguishing between an abaya and a kebaya. To help people with the similar situation, you then decided to develop an AI system to identify the type of clothes using a Convolutional Neural Networks (ConvNet). In order to train the network, you decide to use the Fashion MNIST dataset which is freely available on Pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzzvkxpn8zuo"
      },
      "source": [
        "a) Given the problem, what is the most appropriate loss function to use? Justify your answer. **[5 marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hERYSq8zuo"
      },
      "source": [
        "\n",
        "<span style=\"color:blue\">\n",
        "    ANSWER: Huber Loss. The Huber Loss offers the best of both worlds by balancing the MSE and MAE together. Mean Squared Error, MSE will take the difference between the modelâ€™s predictions and the ground truth, square it out and then average it out across the whole dataset.\n",
        "The result is always positive regardless of the sign of the predicted and ground truth values and a perfect value is 0.0. Mean Absolute Error, MAE refers to the magnitude of difference between the prediction of an observation and the true value of that observation.\n",
        "    For loss values less than delta, use the MSE; for loss values greater than delta, use the MAE. This effectively combines the best of both worlds from the two loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6A4Pmj8zuo"
      },
      "source": [
        "b) Create and train a ConvNet corresponding to the following CNN architecture (with a modification of the final layer to address the number of classes). Please include **[10 marks]**:\n",
        "\n",
        "    1) The dataloader to load the train and test datasets.\n",
        "\n",
        "    2) The model definition (either using sequential method OR pytorch class method).\n",
        "\n",
        "    3) Define your training loop.\n",
        "\n",
        "    4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/LeNet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ue0OHCL8zup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2000e9e0-1d6b-475e-de74-dea84f78fc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "\n",
        "#Numpy is linear algebra lbrary\n",
        "import numpy as np\n",
        "# Matplotlib is a visualizations library \n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "###############################################\n",
        "######## THE REST OF YOUR CODES HERE ##########\n",
        "###############################################\n",
        "\n",
        "# In case you wanna experiment with fashinMNIST dataset\n",
        "\n",
        "transform = transforms.Compose(\n",
        "     [transforms.Resize(32),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "\n",
        "# # batch_size\n",
        "batch_size = 4\n",
        "\n",
        "# # datasets\n",
        "trainset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=True,\n",
        "     transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=False,\n",
        "     transform=transform)\n",
        "\n",
        "# # dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# # constant for classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)\n",
        "\n",
        "#1. DEFINE THE CNN \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5) \n",
        "        #(input channel, output channel, kernel size)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        #(kernel size, stride)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN() # need to instantiate the network to be used in instance method\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ],
      "metadata": {
        "id": "h48MVnO3b4bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80PyCUmlb_Zb",
        "outputId": "c05aff5e-8c57-4feb-d1a0-2fb4d4fdcbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.5358, Accuracy: 80.1833%, \n",
            "\t\tValidation : Loss : 0.4289, Accuracy: 84.6900%, Time: 94.4487s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.3488, Accuracy: 87.3300%, \n",
            "\t\tValidation : Loss : 0.3732, Accuracy: 86.0400%, Time: 98.2392s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3064, Accuracy: 88.7233%, \n",
            "\t\tValidation : Loss : 0.3244, Accuracy: 88.2800%, Time: 90.0100s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2815, Accuracy: 89.4800%, \n",
            "\t\tValidation : Loss : 0.3311, Accuracy: 88.3100%, Time: 89.8540s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2625, Accuracy: 90.2067%, \n",
            "\t\tValidation : Loss : 0.3046, Accuracy: 88.5500%, Time: 90.9421s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2469, Accuracy: 90.8283%, \n",
            "\t\tValidation : Loss : 0.2853, Accuracy: 89.3600%, Time: 91.4058s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.2343, Accuracy: 91.1717%, \n",
            "\t\tValidation : Loss : 0.2865, Accuracy: 89.6300%, Time: 92.8200s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2235, Accuracy: 91.7450%, \n",
            "\t\tValidation : Loss : 0.3159, Accuracy: 88.9100%, Time: 89.8583s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2114, Accuracy: 92.1267%, \n",
            "\t\tValidation : Loss : 0.3067, Accuracy: 89.0900%, Time: 91.1286s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2022, Accuracy: 92.2833%, \n",
            "\t\tValidation : Loss : 0.3025, Accuracy: 89.6000%, Time: 90.9323s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1L-H94eGcdeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e2ab9038-b3ee-459f-ff88-bff68b2cd310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf7UlEQVR4nO3de3hddZ3v8fd3X5I0SZOWUqBXWpERoVcoBU5HpHBGQZDigEq5WVAZOY4o6BHUZwZkZISREabKiJwRBQUqonIRsPoAUhgVaUsvQOGcyq1pC6S3pCVNsi/f88daSXZ2kzZNsrKTrM/refaz1vqt31r7m03Zn/1ba+21zd0REZH4SpS6ABERKS0FgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxFxkQWBmFWb2FzNbbWYvmtk3u+hTbmY/N7P1ZvasmU2Jqh4REelalCOCFuBkd58JzAJONbPji/p8Gtju7u8FbgZujLAeERHpQmRB4IFd4WI6fBR/e20BcGc4fz9wiplZVDWJiMieUlHu3MySwArgvcCt7v5sUZcJwAYAd8+aWQMwBthStJ9LgUsBqqqqjjniiCOiLFtEZNhZsWLFFncf29W6SIPA3XPALDMbBfzazKa5+wu92M/twO0Ac+bM8eXLl/dzpSIiw5uZvdHdugG5asjddwBPAqcWrdoITAIwsxRQC2wdiJpERCQQ5VVDY8ORAGY2Avg74OWibg8BnwrnzwGecN0FT0RkQEV5aGgccGd4niAB3OfuvzGz64Dl7v4Q8CPgp2a2HtgGnBthPSIi0oXIgsDd1wCzu2j/54L5ZuDjUdUgIsNDJpOhrq6O5ubmUpcy6FVUVDBx4kTS6XSPt4n0ZLGISH+oq6tj5MiRTJkyBV1h3j13Z+vWrdTV1TF16tQeb6dbTIjIoNfc3MyYMWMUAvtgZowZM2a/R04KAhEZEhQCPdOb10lBICIScwoCEZF92Lp1K7NmzWLWrFkccsghTJgwoX25tbV1j/5/+MMfOOOMM0pQae/oZLGIyD6MGTOGVatWAXDttddSXV3NV77ylfb12WyWVGrovp1qRCAi0guLFi3ic5/7HMcddxxf/epXe7TNvffey/Tp05k2bRpXXXUVALlcjkWLFjFt2jSmT5/OzTffDMDixYs58sgjmTFjBueeG+1XrIZuhIlILH3z4Rd5aVNjv+7zyPE1XPPRo/Z7u7q6Ov74xz+STCb32XfTpk1cddVVrFixgtGjR/OhD32IBx54gEmTJrFx40ZeeCG4DduOHTsAuOGGG3jttdcoLy9vb4uKRgQiIr308Y9/vEchAPDcc89x0kknMXbsWFKpFOeffz7Lli3jPe95D6+++ipf+MIX+O1vf0tNTQ0AM2bM4Pzzz+dnP/tZ5IedNCIQkSGlN5/co1JVVdXnfYwePZrVq1ezdOlSbrvtNu677z7uuOMOHnnkEZYtW8bDDz/M9ddfz9q1ayMLBI0IREQGwNy5c3nqqafYsmULuVyOe++9lw9+8INs2bKFfD7P2Wefzbe+9S1WrlxJPp9nw4YNzJ8/nxtvvJGGhgZ27dq17yfpJY0IREQi8PjjjzNx4sT25V/84hfccMMNzJ8/H3fn9NNPZ8GCBaxevZqLL76YfD4PwLe//W1yuRwXXHABDQ0NuDuXX345o0aNiqxWG2p3fdYP04jEz7p163j/+99f6jKGjK5eLzNb4e5zuuqvQ0MiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiI7MP8+fNZunRpp7ZbbrmFyy67rNttTjrpJLq61L279lJSEIiI7MPChQtZsmRJp7YlS5awcOHCElXUvxQEIiL7cM455/DII4+0/wjN66+/zqZNm/jABz7AZZddxpw5czjqqKO45pprerX/bdu2cdZZZzFjxgyOP/541qxZA8BTTz3V/gM4s2fPZufOnWzevJkTTzyRWbNmMW3aNJ5++uk+/326xYSIDC2PXQ1vre3ffR4yHU67odvVBxxwAHPnzuWxxx5jwYIFLFmyhE984hOYGddffz0HHHAAuVyOU045hTVr1jBjxoz9evprrrmG2bNn88ADD/DEE09w0UUXsWrVKm666SZuvfVW5s2bx65du6ioqOD222/nwx/+MN/4xjfI5XI0NTX19a/XiEBEpCcKDw8VHha67777OProo5k9ezYvvvgiL7300n7v+5lnnuHCCy8E4OSTT2br1q00NjYyb948rrzyShYvXsyOHTtIpVIce+yx/PjHP+baa69l7dq1jBw5ss9/m0YEIjK07OWTe5QWLFjAFVdcwcqVK2lqauKYY47htdde46abbuK5555j9OjRLFq0iObm5n57zquvvprTTz+dRx99lHnz5rF06VJOPPFEli1bxiOPPMKiRYu48sorueiii/r0PBoRiIj0QHV1NfPnz+eSSy5pHw00NjZSVVVFbW0tb7/9No899liv9v2BD3yAu+++Gwh++P7AAw+kpqaGv/71r0yfPp2rrrqKY489lpdffpk33niDgw8+mM9+9rN85jOfYeXKlX3+2zQiEBHpoYULF/Kxj32s/RDRzJkzmT17NkcccQSTJk1i3rx5PdrP6aefTjqdBuCEE07ghz/8IZdccgkzZsygsrKSO++8EwguUX3yySdJJBIcddRRnHbaaSxZsoTvfOc7pNNpqqurueuuu/r8d0V2G2ozmwTcBRwMOHC7u/9HUZ+TgAeB18KmX7n7dXvbr25DLRI/ug31/tnf21BHOSLIAl9295VmNhJYYWa/d/fiMylPu/sZEdYhIiJ7Edk5Anff7O4rw/mdwDpgQlTPJyIivTMgJ4vNbAowG3i2i9UnmNlqM3vMzAbPr1KLyKAy1H5NsVR68zpFHgRmVg38EviSuzcWrV4JHOruM4HvAQ90s49LzWy5mS2vr6+PtmARGXQqKirYunWrwmAf3J2tW7dSUVGxX9tF+pvFZpYGfgMsdffv9qD/68Acd9/SXR+dLBaJn0wmQ11dXb9eoz9cVVRUMHHixParktqU5GSxmRnwI2BddyFgZocAb7u7m9lcghHK1qhqEpGhKZ1OM3Xq1FKXMWxFedXQPOBCYK2ZrQrbvg5MBnD324BzgMvMLAvsBs51jf1ERAZUZEHg7s8Ato8+3we+H1UNIiKyb7rFhIhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZiLLAjMbJKZPWlmL5nZi2b2xS76mJktNrP1ZrbGzI6Oqh4REelaKsJ9Z4Evu/tKMxsJrDCz37v7SwV9TgMODx/HAT8IpyIiMkAiGxG4+2Z3XxnO7wTWAROKui0A7vLAn4FRZjYuqppERGRPA3KOwMymALOBZ4tWTQA2FCzXsWdYYGaXmtlyM1teX18fVZkiIrEUeRCYWTXwS+BL7t7Ym324++3uPsfd54wdO7Z/CxQRiblIg8DM0gQhcLe7/6qLLhuBSQXLE8M2EREZIFFeNWTAj4B17v7dbro9BFwUXj10PNDg7pujqklERPYU5VVD84ALgbVmtips+zowGcDdbwMeBT4CrAeagIsjrEdERLoQWRC4+zOA7aOPA5+PqgYREdk3fbNYRCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnM9CgIzqzKzRDj/N2Z2ppmloy1NREQGQk9HBMuACjObAPwOuBD4SVRFiYjIwOlpEJi7NwF/D/ynu38cOCq6skREZKD0OAjM7ATgfOCRsC0ZTUkiIjKQehoEXwK+Bvza3V80s/cAT0ZXloiIDJQeBYG7P+XuZ7r7jeFJ4y3ufvnetjGzO8zsHTN7oZv1J5lZg5mtCh//3Iv6RUSkj3p61dA9ZlZjZlXAC8BLZva/97HZT4BT99HnaXefFT6u60ktIiLSv3p6aOhId28EzgIeA6YSXDnULXdfBmzrW3kiIhK1ngZBOvzewFnAQ+6eAbwfnv8EM1ttZo+ZWbdXIZnZpWa23MyW19fX98PTiohIm54GwQ+B14EqYJmZHQo09vG5VwKHuvtM4HvAA911dPfb3X2Ou88ZO3ZsH59WREQK9fRk8WJ3n+DuH/HAG8D8vjyxuze6+65w/lGCUceBfdmniIjsv56eLK41s++2HZ4xs38nGB30mpkdYmYWzs8Na9nal32KiMj+S/Ww3x0EVwt9Ily+EPgxwTeNu2Rm9wInAQeaWR1wDZAGcPfbgHOAy8wsC+wGznX3/jjvICIi+6GnQXCYu59dsPxNM1u1tw3cfeE+1n8f+H4Pn19ERCLS05PFu83sb9sWzGwewad4EREZ4no6IvgccJeZ1YbL24FPRVOSiIgMpB4FgbuvBmaaWU243GhmXwLWRFmciIhEb79+oSy85LPt+wNXRlCPiIgMsL78VKX1WxUiIlIyfQkCXeopIjIM7PUcgZntpOs3fANGRFKRiIgMqL0GgbuPHKhCRESkNPpyaEhERIYBBYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiLrIgMLM7zOwdM3uhm/VmZovNbL2ZrTGzo6OqRUREuhfliOAnwKl7WX8acHj4uBT4QYS1iIhINyILAndfBmzbS5cFwF0e+DMwyszGRVWPiIh0rZTnCCYAGwqW68K2PZjZpWa23MyW19fXD0hxIiJxMSROFrv77e4+x93njB07ttTliIgMK6UMgo3ApILliWGbiIgMoFIGwUPAReHVQ8cDDe6+uYT1iIjEUiqqHZvZvcBJwIFmVgdcA6QB3P024FHgI8B6oAm4OKpaRESke5EFgbsv3Md6Bz4f1fOLiEjPDImTxSIiEh0FgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzMUmCNwddy91GSIig06kQWBmp5rZK2a23syu7mL9IjOrN7NV4eMzUdWypq6Bjyx+hp/+6XUamzNRPY2IyJCTimrHZpYEbgX+DqgDnjOzh9z9paKuP3f3f4yqjjZNrTkM+KcHX+RfH32Zj84cx3nHHcrMibWYWdRPLyIyaEUWBMBcYL27vwpgZkuABUBxEAyIEw4bwyOX/y1r6hq459k3eWj1Ju5bXseR42pYeNxkzpo1npEV6VKUJiJSUlEeGpoAbChYrgvbip1tZmvM7H4zmxRhPZgZMyeN4sZzZvCXb5zCv5w1DQf+6YEXOO5fH+fqX65h9YYdOpcgIrES5YigJx4G7nX3FjP7B+BO4OTiTmZ2KXApwOTJk/vliUdWpLnw+EO54LjJrK5r4J5n3+DBVZtY8twGjhpfw3nHTWbBrAlUl5f6JRIRiZZF9enXzE4ArnX3D4fLXwNw92930z8JbHP32r3td86cOb58+fL+LheAxuYMDz6/kbuffZOX39pJZVmSBbPGc97cQ5k+ca9liYgMama2wt3ndLUuyo+7zwGHm9lUYCNwLnBeUWHj3H1zuHgmsC7CevappiLNhSdM4YLjD2XVhh3c8+yb/Pr5jdz7lw1Mm1DDeXMP5cxZ4zVKEJFhJbIRAYCZfQS4BUgCd7j79WZ2HbDc3R8ys28TBEAW2AZc5u4v722fvR4RZFsgn4Oyyv3arLE5wwPPb+SecJRQVZbkzFkTOP+4yUyboFGCiAwNexsRRBoEUeh1EKz7Dfz8fKgcA7WTYNSkYFo7CWondixXjoEuLid1d54PRwm/WbOJ5kyeGRNrWTh3MmfOHE+VRgkiMogpCADq/y+sexAa6mDHhmDasAEyTZ37pUZ0Dob20JgYzNeMp6GV9lHCK28Ho4QFsydw3lyNEkRkcFIQdMcddm+HHW8GodAeEhs6lt+t77yNJWDkOKidiNdO4i0by3/Xj2DpxjRvZA/ggHHv4azjj+CjGiWIyCCiIOiLzG5o2AgNb3YeSTTUBQHSuBHy2U6bNHglmxmL107koEmHM2b8YeEoY3IwrToIErG5zZOIDAKlumpoeEiPgAPfGzy6ks/Brrfbg8F3bKBl43rydX8l2bCBdMNKeHF3520S6WBUMfIQqBkHI8d3TEceAjXjg/X7eWJbRKQ3FAR9lUgGb9w142HSXAw4KHw0NGX45fN1PPDndbRseYPDyrZz6sQMx4xuojZbT8Xud0i+/RKsfxxad+2574raMDDGdYRDzbjObVVjgxpERHpJh4YGgLuz4o3twRVHazfTms23rxtZkWJsdTmTKrMcNqKRQ1MNjE/u4CC2Mzq/hZrWeipb6kk3vYXtegc813nnloTqgzsCoiYcVRSOMmrGQfnIAf6rRWQw0TmCQWRHUysr39xO/c4WtuxqpX5nC/W7Wtiys4Utu4K2ht1d3ya7tjzB4VVNHDZiJ1PLGpmQ3MEhto0D8tsYld1CZcs7lDe9TaK1cc+Ny6oLRhRhWFTUBIe28tmCaRY83zGfz3Ws84I++YI+nuvbflJlUF4ThFVFbTAtrwnqa5vf1zqNikT2SucIBpFRlWWcfMTBe+3Tks2xdVdrGAwtbNnZSv2uljA8WnhjVwvLd3YfGiNoZmr5Tv5mxE6mljcwKdXAIbadsb6NUdu3UP32XylvfodEvmNbtySWSAVvqG1Ta5tPBSe32+atoE+isE8KUhUF63qyn2TwZb+WRmjZCc2NwUn4lsZgvmXnnqOgrpRV9yJAipbTI7r8Don0A/fwv29DF48d3bdnmiGZDv69JNPB+bU9llPdtPeiX7Ks63WJVHDFoCXCf9OJ4N+KJbtoL3p0ai/cdvD8W1MQDELlqSTjR41g/KgR++zbms2z9d2OkGgLjS1hcPxpVwsPh6Gyo6njjd/IU0aWDCnyGKlEguqKFFXJFCMrUlSXp6gqT1FdkWJk23z5vtdVladIJ/vxiij34LsebSHR0tg5JDrNN3T0a24ITuC3rcu824MnM0hXBifp0yMgXRVMy8JpurJgfRfz7X0ru95PunLoXi3mDq3vdvOGXfhm3t2bekMwQtybsjC0K2phxKjgezupcshlgtFjLgP5DOSykG3uuj2f6XqZQXjko1NgFAZKV23h/LGfhr+9ot9LURAMcWWpBONqRzCutuehsWVnEAzbm1p5tyXLzpYsu5qzneZ3tWTZ0dTKhu1N7evebe3BJ3OgPJXoHBgFIVEdBkh1WTgtT1FZlqKyLElFOkllWZIRZUlGpINpZVmSilQlibKq4HBWb+WyHaOOwtFHW4A0NwaBk9kdvOFldgfhkdkNrU3BlWFt85nwkW3e/zpSFWFIdBUu4TIGePDmC32cb2ty2t8MezKfy4QhW/BGXnSZ9B7SVR1v5BW1UH0IjD2ic1unx6iO+fKa4BN7VPK5goDoLkBauw+TfC4IMs8Fr5PnC9ra2sP5fL6b9oJt29o77SNf1Ld4Hw6jp0Ty8igIYmR/QqMrubzzbuueoVEcIF2t27SjudO6whPmPVGeSgQhEQbEiLIklekUFWVJKsO29iBJd4RJW7B0rBvJiLJRjKhMUjkq1d43nbT9/6W6fL4jPApDo32+LVDC4CgMkcL5zG5o2tbRt40ZQSj0xzzBcvufaEXtRfOWhMoD4YDDOn9K39ubeXIQ/7BT22FMKkpdyaCkIJAeSyaMmoo0Nf3wS26t2XwQCs1ZmjJZdrfmgkcmeDS15mgOp+3tRdOm1iwNuzO81bA7bM+zuzVLUya3xwfinvxtFakE5ekkFakEFekkZeG0PJxWpBOUp7qeBv1GUJ6ubu9fnk5QUZnstI/iaTIxeI4TS3wpCKQkylIJylJljK4q6/d9uzst2XxHkBSFSKeQyeSC8GjN0ZLN05LN0ZwJtm3bR0s2z46m1k7LhdN8Hw4/p5PWKVTK0wnKkgnKUwnSyUT4OgVtbfPt6wrXp3q2XVkySTplnduTyfZ5BVM8KQhk2DGz8BN8klERfznb3cnkvD1Aupq2ZLoPmK6mrdk8rbl8MA1HTi1FbYXz2b4kUZGE0SlA0sm2h7UHTKflZIJUwXw6mSCdKloO28qSCVIJI10QZO37ShUtFz1Xp1BLBoG134fypFsKApE+MDPKUsEb1cgSHX7O5z0IhsKgKAiLlnA5k9szRFqKtskU7ieXJxMGTdt8JpcnkwuWd7Vkg+Wst2+XzXXMt/XN9WNQtTGjUzC0jWg6jZQ6tRtlqWQ4AusIncLtyrvYR1t7uj3wgivs2sIvlbD2dalE8DypgvahMsJSEIgMcYmEUZEIRkCDUS7vYSgEwdAWSJlcGDLZonXtoeO05nJksk5L2Naa6xxqLQXznUPMac22nUfyzgFYtI/+HFEVSxikkgnSiSAgugqSjvbOfdpGRh3bG/PfdxCnTR/X73UqCEQkUsmEkRzEQVU8osoUjpLa25xsLk8mH05zTjbfMQLKFrW3hVo252TCfoXbB+1d76s5kyeTy7Y/Zzbfsa9Dx1RF8hooCEQk1gb7iGogDNGvOYqISH9REIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMRcpEFgZqea2Stmtt7Mru5ifbmZ/Txc/6yZTYmyHhER2VNkQWBmSeBW4DTgSGChmR1Z1O3TwHZ3fy9wM3BjVPWIiEjXohwRzAXWu/ur7t4KLAEWFPVZANwZzt8PnGL6tQkRkQEV5d1HJwAbCpbrgOO66+PuWTNrAMYAWwo7mdmlwKXh4i4ze6WXNR1YvO+Y0+vRmV6PDnotOhsOr8eh3a0YErehdvfbgdv7uh8zW+7uc/qhpGFBr0dnej066LXobLi/HlEeGtoITCpYnhi2ddnHzFJALbA1wppERKRIlEHwHHC4mU01szLgXOChoj4PAZ8K588BnnD36H43TkRE9hDZoaHwmP8/AkuBJHCHu79oZtcBy939IeBHwE/NbD2wjSAsotTnw0vDjF6PzvR6dNBr0dmwfj1MH8BFROJN3ywWEYk5BYGISMzFJgj2dbuLODGzSWb2pJm9ZGYvmtkXS11TqZlZ0syeN7PflLqWUjOzUWZ2v5m9bGbrzOyEUtdUKmZ2Rfj/yAtmdq+ZVZS6pijEIgh6eLuLOMkCX3b3I4Hjgc/H/PUA+CKwrtRFDBL/AfzW3Y8AZhLT18XMJgCXA3PcfRrBRS9RX9BSErEIAnp2u4vYcPfN7r4ynN9J8D/6hNJWVTpmNhE4HfivUtdSamZWC5xIcEUf7t7q7jtKW1VJpYAR4fecKoFNJa4nEnEJgq5udxHbN75C4R1fZwPPlraSkroF+CqQL3Uhg8BUoB74cXio7L/MrKrURZWCu28EbgLeBDYDDe7+u9JWFY24BIF0wcyqgV8CX3L3xlLXUwpmdgbwjruvKHUtg0QKOBr4gbvPBt4FYnlOzcxGExw5mAqMB6rM7ILSVhWNuARBT253EStmliYIgbvd/VelrqeE5gFnmtnrBIcMTzazn5W2pJKqA+rcvW2EeD9BMMTR/wRec/d6d88AvwL+R4lrikRcgqAnt7uIjfBW3z8C1rn7d0tdTym5+9fcfaK7TyH4d/GEuw/LT3094e5vARvM7H1h0ynASyUsqZTeBI43s8rw/5lTGKYnzofE3Uf7qrvbXZS4rFKaB1wIrDWzVWHb19390RLWJIPHF4C7ww9NrwIXl7ieknD3Z83sfmAlwZV2zzNMbzWhW0yIiMRcXA4NiYhINxQEIiIxpyAQEYk5BYGISMwpCEREYk5BIEOameXMbFXBo9++BWtmU8zshR70u9bMmszsoIK2XQNZg0hfxOJ7BDKs7Xb3WaUuAtgCfBm4qtSFFDKzlLtnS12HDG4aEciwZGavm9m/mdlaM/uLmb03bJ9iZk+Y2Roze9zMJoftB5vZr81sdfhou5VA0sz+T3hP+t+Z2YhunvIO4JNmdkBRHZ0+0ZvZV8zs2nD+D2Z2s5ktD+/7f6yZ/crM/p+ZfatgNykzuzvsc7+ZVYbbH2NmT5nZCjNbambjCvZ7i5ktJ7i9tsheKQhkqBtRdGjokwXrGtx9OvB9gjuMAnwPuNPdZwB3A4vD9sXAU+4+k+DeOm3fPD8cuNXdjwJ2AGd3U8cugjDY3zfeVnefA9wGPAh8HpgGLDKzMWGf9wH/6e7vBxqB/xXeK+p7wDnufkz43NcX7LfM3ee4+7/vZz0SQzo0JEPd3g4N3VswvTmcPwH4+3D+p8C/hfMnAxcBuHsOaAjvPvmau7fdhmMFMGUvtSwGVpnZTftRf9s9r9YCL7r7ZgAze5XgRok7gA3u/t9hv58R/FjKbwkC4/fBbXBIEtwquc3P96MGiTkFgQxn3s38/mgpmM8B3R0awt13mNk9BJ/q22TpPPIu/qnDtv3ni54rT8f/n8W1O2AEwdHdz0i+212dIsV0aEiGs08WTP8Uzv+Rjp8bPB94Opx/HLgM2n+/uLaXz/ld4B/oeBN/GzjIzMaYWTlwRi/2Obngd4PPA54BXgHGtrWbWdrMjuplzRJzCgIZ6orPEdxQsG60ma0hOG5/Rdj2BeDisP1COo7pfxGYb2ZrCQ4B9eo3nN19C/BroDxczgDXAX8Bfg+83IvdvkLwu9LrgNEEPxrTCpwD3Ghmq4FVDNN75Uv0dPdRGZbCH5qZE74xi8heaEQgIhJzGhGIiMScRgQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJz/x8khRMd+HkDCQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sdvVqk25ch6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b5895870-48e7-4ac0-cf48-51b1b0d9f302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxU9Znv8c/T1dU7srYboKARxQ0dCBJ13Ih3dCSQxBWvk1GjJE40aszcMcZJjNHXTYxbzHhVXGOGNC4zziVelXEniRoFF1TcAUMryirQ9FLbc/84p7urm26ohj5d0Of7fr3qVef3O7869Vgtz3PqnFO/Y+6OiIjEV0mxAxARkeJSIRARiTkVAhGRmFMhEBGJORUCEZGYUyEQEYm5yAqBmd1jZivM7K1u1puZ3WJmH5rZQjP7m6hiERGR7kX5jeA+4ITNrD8R2Cd8zABuizAWERHpRmSFwN3nAWs2M2QacL8HXgIGmdluUcUjIiJdKy3iew8HluW168O+5Z0HmtkMgm8NVFdXj99vv/36JEARkf5iwYIFq9y9tqt1xSwEBXP3mcBMgAkTJvj8+fOLHJGIyI7FzD7ubl0xrxr6BBiZ1x4R9omISB8qZiGYA3wrvHpoErDO3Tc5LCQiItGK7NCQmdUBxwDDzKwe+CmQBHD324HHgL8HPgQagXOiikVERLoXWSFw9+lbWO/A96J6fxERKYx+WSwiEnMqBCIiMadCICIScyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBCIiMadCICIScyoEIiIxt0NMQy0i0pvcnUzOSWdzpDNOSzZLOuukMzlS2RypTC5Yl3VSmRyZXA53yOacnLc+OrazOYLlXLiubdnJ5jx4fetrc3nj817v4dis+6bvl4OvHzqcr+w9tNc/DxUCEekz6WyOpnSW5nSW5lSO5kyWplTYzuRoSmVpyetrCRNyKkzIQXIOHsG6IHkHY9oTeCpM8J3701kP1mWDxL49MIOEGSVmlJRAiVnQLjFKDBIlhoV9h+01JJIYVAhEYiaXc9K5IClmWpNmuJzO5khlgmTZnM7SlM7SEj43p9v7Wpeb02HSzrS388e0JvbmdFAAsrmtz75liRKSCaOstIRkIniUlZYE/aXW1lddXsqgtrGJ4DlvfDJRQlnCOrSTpSWUd9pO67ZLS4zSRElbUi4Jk3YiTNQlYV8iP5GXWFuCb0vkreM7vd7MevGvu3VUCEQilgn3gpvCveCmdJbGVKZtz7gplaMxlQmTaK5tjzWTbd+jbW1vkrS7WU6Hr+1qeVuScb6KZAkVyQSVyQQVyQTlpSVUliWoKE2w84Bk2/qKZNBXWVYSPicob3tdSdvr88fnb7OsNEjG20PC7K9UCCTWcjmnKZ1lYypDcypHYzpDUyrblqQbU+2HKZpa2+kszeFzUzpHU5jUg9d1bLcm9q0R7ImGe66Jki6Xk+GebzJRQlVZ+3IyHFPI67pbrkwmqChLtCXvimReIi8tUWLuR1QIZIeRCo8hb0xlaExl2NgSLrc+p7JsbAmf8/vz1geP4LWNYV9Pte75ViYT7c/JBNXlpQyrae+rSCaoyhvXuqdbVRYk2Mr8dt6Y1sMRJSVKtNI3VAgkEulsjo0tGTaGyTl4BAk5v78xXM5Pzm3PqWzb+qZUtkd71uWlwbHiqrIE1WWlVJUHCXdIdRnVZQmqykuD57JSqssTVJYF7da94Mq8JN6apKvCveNeS9C5HKQaoGV98PxFQ/CcS0MuC9k05DIdH537suHYXCZ8XQaymY7tDtva0razwRgMyqqhrArKasLl6vblZFXHdtty1abj4vLNwT34HNNNkGmBTHP7I9263AKZcH26qZu+zbz2iO/D2K/1eugqBIK7t+1Fb2xpT9yNqSwNLcHed0NLkJQbWvewWzLt41Ptib41kfckaXeVmAdVlTF8cNjuYn2H5zDRtyX8ZILSREQ/kclmoGkDtDRAy4YwkW/IW27tL2BMemPvxmYJSCShpLTjI5GEkgSUhOsSrevyxiYr8/oSgEOqEVIboXFZEHc6bKcaehLUZgrH5opMuJxIBsXJs52ecz3oz3U9rifbyKbzknResk43d0zavnWHAds+q2QllJZDaetzBSQrguey6uDvEwEVgn6oOZ1lVUMLqxpSrNrQEi6H7bzl9U3pIOGnswVfSldWWkJ1WXAYpDXx1pSXsvOA8ra+6jBpV5cHybptbFtf2F9WSmUy3MN2z9uzzbTvyXbea82lIdfUaUwWNmZgQ3evK6CdTQcJrkMy7yKBZ5oK+6AS5VBeEyS08p2C5epaGLJX2D8AygfkjQnbySpIlOUl67zk3JbkO7fDR1/teedywefQWhRSeQUitbF9ua1w5K9rDJab18H6TzuOzTRHH7slgs/OSjoulyTy2gkoKenYLi0LknFpBVQNC5J0Mi9Ztz6SFR3bHfq6SfCtj0SyaN+eVAh2EBtbMpsm9A35ib098W9oybS9LkGW4baK0fYZ+yZXcmzZCkbbZwz3z6jyjZSUgZWD4RiGmWMQPCyvHwcDay0YOYdmoKm1I3z2rWh7dhv3pHpJaWWn5LwT7LR7XqLuKoHvtGkyL6sJEkd/VVLSvhfPzr233Wwm+JaUXzyymU2TcnfJupB+6ZIKQZG4OxtaMuEee15C39DCyoZOCX5DiqZ01yc1B1Ul2bm6lH0r13H0wBWMHvw5w3PL2Tldz8CmZVRurKckl857RXWwVzpkAlS1/jgl3Atp2xvZ1jY9G2+JvD3bRMe93G7bPXlNp77W5LDJGCWKokqUQmIgVAwsdiSxo0LQhxZ9up47/7iYl5esYWVDC6nMpnvBZjC0uoxhNeUMqylnzz2qwuUkI0vWMNyXU5uqZ1DTMio3LKVk7WJYuxQ2pNo3kqwKkv2Ig2Do12HI3jB07+C5Zuf4nLwTkYKoEETM3Xnho9Xc/vxH/PGDVVSVJfjq2F3YbWBFkOAHtCf9YdVJhuRWk1i7GFZ/BGs+gtWL4eOPYM0SyLa0b7i0Ikj2w8bAvifmJfu9YMBuSvYiUjAVgohksjkee+szZs77iLc+Wc+wmnL++e/25ayJIxmYXRMm+Y9g1Ufw3kewZnGQ7PNPRibKYcjoIMnvc3x4SCdM+AN216EMEekVKgS9Id0EG1fCxpU0r/ucBW+/z5vvfUBp82ourmjkoBEpdi5ZT8mCVTBvVXBytFVJsj3Z73UsDM1L9jsNDy/lExGJjgpBV3I5aFrbltzZuAI2rmpvN6zMW7cquGY8VAEcET4y5VUkamqx8lqo2RNGjA8uIRywW7B3P3RvGDhSyV5Eiio+hSDdHCT0Dkk8TOQbV+Qth8/exVU6VhJcQ1xdC9XDYPh41icG8dKKBM/XO59lB7DXnqP52uHjOHjfvSktq+77/04RkR6KTyF48d/gmZ9v2l9WEyT16p1h0J4wPNxrr66Fmtr25epaqBzctvf+xrIvuGPeRzzx1meUlpTwjUOH86OjRvOlnQf08X+YiMi2iU8hGPN3ULNLxwRfNSz4mXuB3J3n3l3BHfM+4qXFaxhQUcp3jt6bcw4fxc47VUQYvIhIdOJTCHY9KHhshVQmxx/e+JSZ8xbz3ucb2G1gBT/++7GcMXEkAyqimftDRKSvxKcQbIUNzWlmv7yMu/+0hM/WN7PvLgO44dRxfG3c7pSV6tJNEekfVAi6sGJ9M/f8eSmz/vIxG5ozTNprCP/75IM4ZkytbsYhIv2OCkGeD1c0cOe8xTzy2idkcjlOPHA3Zhy1F+NGDip2aCIikVEhAOYvXcPtzy/mqXc+p7y0hNO+PILzjtyLUcN0+aeI9H+xLQS5nPPkO59zx/Mf8epfv2BQVZLvT96Hf/zKngytKS92eCIifSbSQmBmJwC/BhLAXe7+i07r9wB+CwwKx1zu7o9FGVNzOssjr33CnfMWs3jVRkYMruRnUw/g1AkjqCqLbV0UkRiLLPOZWQK4FTgeqAdeMbM57r4ob9iVwIPufpuZ7Q88BoyKIp51jWn+/S8fc++fl7KqoYUDh+/Eb6YfyokH7hrdbQ1FRHYAUe4CTwQ+dPfFAGY2G5gG5BcCB3YKlwcCn0YVzN1/XsItT3/AUWNq+c5Re3H43kN1BZCICNEWguHAsrx2PXBYpzFXAf9tZhcB1cBXu9qQmc0AZgDsscceWxXM2YeP4oQDdmX/3Xfa8mARkRgp9jGR6cB97j4C+Hvgd2a2SUzuPtPdJ7j7hNra2q16oyHVZSoCIiJdiLIQfAKMzGuPCPvyfRt4EMDdXySYxXlYhDGJiEgnURaCV4B9zGy0mZUBZwBzOo35KzAZwMzGEhSClRHGJCIinURWCNw9A1wIzAXeIbg66G0zu9rMpobDLgPON7M3gDrgbHf3qGISEZFNRXrhfPibgMc69f0kb3kRwc28RESkSIp9slhERIpMhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5iItBGZ2gpm9Z2Yfmtnl3Yw5zcwWmdnbZvb7KOMREZFNlUa1YTNLALcCxwP1wCtmNsfdF+WN2Qf4EXCEu681s52jikdERLoW5TeCicCH7r7Y3VPAbGBapzHnA7e6+1oAd18RYTwiItKFKAvBcGBZXrs+7Ms3BhhjZn82s5fM7ISuNmRmM8xsvpnNX7lyZUThiojEU7FPFpcC+wDHANOBO81sUOdB7j7T3Se4+4Ta2to+DlFEpH/bYiEws6+Z2dYUjE+AkXntEWFfvnpgjrun3X0J8D5BYRARkT5SSII/HfjAzK4zs/16sO1XgH3MbLSZlQFnAHM6jfkvgm8DmNkwgkNFi3vwHiIiso22WAjc/SzgUOAj4D4zezE8Zj9gC6/LABcCc4F3gAfd/W0zu9rMpobD5gKrzWwR8Czwz+6+ehv+e0REpIfM3QsbaDYU+AfgEoLE/iXgFnf/TXThbWrChAk+f/78vnxLEZEdnpktcPcJXa0r5BzBVDN7BHgOSAIT3f1EYBxwWW8GKiIifa+QH5SdDNzk7vPyO9290cy+HU1YIiLSVwopBFcBy1sbZlYJ7OLuS9396agCExGRvlHIVUMPAbm8djbsExGRfqCQQlAaThEBQLhcFl1IIiLSlwopBCvzLvfEzKYBq6ILSURE+lIh5wi+C8wys38DjGD+oG9FGpWIiPSZLRYCd/8ImGRmNWG7IfKoRESkzxR0PwIzOwk4AKgwMwDc/eoI4xIRkT5SyA/KbieYb+gigkNDpwJ7RhyXiIj0kUJOFh/u7t8C1rr7z4CvEEwOJyIi/UAhhaA5fG40s92BNLBbdCGJiEhfKuQcwR/Cm8X8CngVcODOSKMSEZE+s9lCEN6Q5ml3/wL4DzN7FKhw93V9Ep2IiERus4eG3D0H3JrXblEREBHpXwo5R/C0mZ1srdeNiohIv1JIIfgOwSRzLWa23sw2mNn6iOMSEZE+Usgvizd7S0oREdmxbbEQmNlRXfV3vlGNiIjsmAq5fPSf85YrgInAAuC4SCISEZE+Vcihoa/lt81sJHBzZBGJiEifKuRkcWf1wNjeDkRERIqjkHMEvyH4NTEEheMQgl8Yi4hIP1DIOYL5ecsZoM7d/xxRPCIi0scKKQQPA83ungUws4SZVbl7Y7ShiYhIXyjol8VAZV67EngqmnBERKSvFVIIKvJvTxkuV0UXkoiI9KVCCsFGM/ub1oaZjQeaogtJRET6UiHnCC4BHjKzTwluVbkrwa0rRUSkHyjkB2WvmNl+wL5h13vuno42LBER6SuF3Lz+e0C1u7/l7m8BNWb2T9GHJiIifaGQcwTnh3coA8Dd1wLnRxeSiIj0pUIKQSL/pjRmlgDKogtJRET6UiEni58AHjCzO8L2d4DHowtJRET6UiGF4F+AGcB3w/ZCgiuHRESkH9jioaHwBvZ/AZYS3IvgOOCdQjZuZieY2Xtm9qGZXb6ZcSebmZvZhMLCFhGR3tLtNwIzGwNMDx+rgAcA3P3YQjYcnku4FTieYOrqV8xsjrsv6jRuAHAxQbEREZE+trlvBO8S7P1Pcfcj3f03QLYH254IfOjui909BcwGpnUx7ufAL4HmHmxbRER6yeYKwTeB5cCzZnanmU0m+GVxoYYDy/La9WFfm3DqipHu/v82tyEzm2Fm881s/sqVK3sQgoiIbEm3hcDd/8vdzwD2A54lmGpiZzO7zcz+x7a+sZmVADcCl21prLvPdPcJ7j6htrZ2W99aRETyFHKyeKO7/z68d/EI4DWCK4m25BNgZF57RNjXagBwIPCcmS0FJgFzdMJYRKRv9eiexe6+Ntw7n1zA8FeAfcxstJmVAWcAc/K2tc7dh7n7KHcfBbwETHX3+V1vTkREorA1N68viLtngAuBuQSXmz7o7m+b2dVmNjWq9xURkZ4p5AdlW83dHwMe69T3k27GHhNlLCIi0rXIvhGIiMiOQYVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTmVAhERGJOhUBEJOYiLQRmdoKZvWdmH5rZ5V2s/4GZLTKzhWb2tJntGWU8IiKyqcgKgZklgFuBE4H9gelmtn+nYa8BE9z9YOBh4Lqo4hERka5F+Y1gIvChuy929xQwG5iWP8Ddn3X3xrD5EjAiwnhERKQLURaC4cCyvHZ92NedbwOPd7XCzGaY2Xwzm79y5cpeDFFERLaLk8VmdhYwAfhVV+vdfaa7T3D3CbW1tX0bnIhIP1ca4bY/AUbmtUeEfR2Y2VeBHwNHu3tLhPGIiEgXovxG8Aqwj5mNNrMy4AxgTv4AMzsUuAOY6u4rIoxFRES6EVkhcPcMcCEwF3gHeNDd3zazq81sajjsV0AN8JCZvW5mc7rZnIiIRCTKQ0O4+2PAY536fpK3/NUo319ERLYs0kLQV9LpNPX19TQ3Nxc7lNirqKhgxIgRJJPJYociIgXqF4Wgvr6eAQMGMGrUKMys2OHElruzevVq6uvrGT16dLHDEZECbReXj26r5uZmhg4dqiJQZGbG0KFD9c1MZAfTLwoBoCKwndDfQWTH028KgYiIbB0Vgl6wevVqDjnkEA455BB23XVXhg8f3tZOpVLdvu6SSy5h+PDh5HK5PoxWRKSjfnGyuNiGDh3K66+/DsBVV11FTU0NP/zhD9vWZzIZSks7ftS5XI5HHnmEkSNH8vzzz3PsscdGEltX7y0ikq/fZYif/eFtFn26vle3uf/uO/HTrx3Qo9ecffbZVFRU8Nprr3HEEUdw4403dlj/3HPPccABB3D66adTV1fXVgg+//xzvvvd77J48WIAbrvtNg4//HDuv/9+rr/+esyMgw8+mN/97necffbZTJkyhVNOOQWAmpoaGhoaeO655/jXf/1XBg8ezLvvvsv777/P17/+dZYtW0ZzczMXX3wxM2bMAOCJJ57giiuuIJvNMmzYMJ588kn23XdfXnjhBWpra8nlcowZM4YXX3wRzfMk0j/1u0KwPamvr+eFF14gkUhssq6uro7p06czbdo0rrjiCtLpNMlkku9///scffTRPPLII2SzWRoaGnj77be55ppreOGFFxg2bBhr1qzZ4nu/+uqrvPXWW22Xcd5zzz0MGTKEpqYmvvzlL3PyySeTy+U4//zzmTdvHqNHj2bNmjWUlJRw1llnMWvWLC655BKeeuopxo0bpyIg0o/1u0LQ0z33KJ166qldFoFUKsVjjz3GjTfeyIABAzjssMOYO3cuU6ZM4ZlnnuH+++8HIJFIMHDgQO6//35OPfVUhg0bBsCQIUO2+N4TJ07scC3/LbfcwiOPPALAsmXL+OCDD1i5ciVHHXVU27jW7Z577rlMmzaNSy65hHvuuYdzzjln2z4IEdmu9btCsD2prq7usn/u3Ll88cUXHHTQQQA0NjZSWVnJlClTerT90tLSthPNuVyuw4np/Pd+7rnneOqpp3jxxRepqqrimGOO2ey1/iNHjmSXXXbhmWee4eWXX2bWrFk9iktEdiy6aqgI6urquOuuu1i6dClLly5lyZIlPPnkkzQ2NjJ58mRuu+02ALLZLOvWreO4447joYceYvXq1QBth4ZGjRrFggULAJgzZw7pdLrL91u3bh2DBw+mqqqKd999l5deegmASZMmMW/ePJYsWdJhuwDnnXceZ511VrffakSk/1Ah6GONjY088cQTnHTSSW191dXVHHnkkfzhD3/g17/+Nc8++ywHHXQQ48ePZ9GiRRxwwAH8+Mc/5uijj2bcuHH84Ac/AOD888/n+eefZ9y4cbz44ovdfgM54YQTyGQyjB07lssvv5xJkyYBUFtby8yZM/nmN7/JuHHjOP3009teM3XqVBoaGnRYSCQGzN2LHUOPTJgwwefPn9+h75133mHs2LFFiqh/mj9/Ppdeeil//OMfe/xa/T1Etj9mtsDdJ3S1TucIZBO/+MUvuO2223RuQCQmdGhINnH55Zfz8ccfc+SRRxY7FBHpAyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBL3g2GOPZe7cuR36br75Zi644IJuX3PMMcfQ+TLYVqtWrSKZTHL77bf3apwiIl1RIegF06dPZ/bs2R36Zs+ezfTp07dqew899BCTJk2irq6uN8LrViaTiXT7IrJj6H+/I3j8cvjszd7d5q4HwYm/6Hb1KaecwpVXXkkqlaKsrIylS5fy6aef8rd/+7dccMEFvPLKKzQ1NXHKKafws5/9bItvV1dXxw033MCZZ55JfX09I0aMAOhyKuqupq3efffdmTJlCm+99RYA119/PQ0NDVx11VUcc8wxHHLIIfzpT39i+vTpjBkzhmuuuYZUKsXQoUOZNWsWu+yyCw0NDVx00UXMnz8fM+OnP/0p69atY+HChdx8880A3HnnnSxatIibbrppWz9hESmi/lcIimDIkCFMnDiRxx9/nGnTpjF79mxOO+00zIxrr72WIUOGkM1mmTx5MgsXLuTggw/udlvLli1j+fLlTJw4kdNOO40HHniAyy67rNupqLuatnrt2rWbjTeVSrUdllq7di0vvfQSZsZdd93Fddddxw033MDPf/5zBg4cyJtvvtk2LplMcu211/KrX/2KZDLJvffeyx133NFLn6KIFEv/KwSb2XOPUuvhodZCcPfddwPw4IMPMnPmTDKZDMuXL2fRokWbLQQPPPAAp512GgBnnHEG5557LpdddhnPPPNMl1NRdzVt9ZYKQf6cQvX19Zx++uksX76cVCrVNiX1U0891eFw1+DBgwE47rjjePTRRxk7dizpdLptBlUR2XHpHEEvmTZtGk8//TSvvvoqjY2NjB8/niVLlnD99dfz9NNPs3DhQk466aTNTv8MwWGh++67j1GjRjF16lQWLlzIBx980KNY8qenBjZ5z/zJ6S666CIuvPBC3nzzTe64444txnfeeedx3333ce+992pCOpF+QoWgl9TU1HDsscdy7rnntp0kXr9+PdXV1QwcOJDPP/+cxx9/fLPbeP/992loaOCTTz5pm6L6Rz/6EXV1dd1ORd3VtNW77LILK1asYPXq1bS0tPDoo492+57r1q1j+PDhAPz2t79t6z/++OO59dZb29qt3zIOO+wwli1bxu9///utPhkuItsXFYJeNH36dN544422BDlu3DgOPfRQ9ttvP84880yOOOKIzb6+rq6Ob3zjGx36Tj75ZOrq6rqdirqraauTySQ/+clPmDhxIscffzz77bdft+951VVXceqppzJ+/Pi2w04AV155JWvXruXAAw9k3LhxPPvss23rTjvtNI444oi2w0UismPTNNTSY1OmTOHSSy9l8uTJXa7X30Nk+7O5aaj1jUAK9sUXXzBmzBgqKyu7LQIisuPpf1cNSWQGDRrE+++/X+wwRKSX9ZtvBDvaIa7+Sn8HkR1PvygEFRUVrF69WkmoyNyd1atXU1FRUexQRKQH+sWhoREjRlBfX8/KlSuLHUrsVVRUtE2JISI7hn5RCJLJZNsvYkVEpGciPTRkZieY2Xtm9qGZXd7F+nIzeyBc/xczGxVlPCIisqnICoGZJYBbgROB/YHpZrZ/p2HfBta6+5eAm4BfRhWPiIh0LcpvBBOBD919sbungNnAtE5jpgGt8xo8DEw2M4swJhER6STKcwTDgWV57XrgsO7GuHvGzNYBQ4FV+YPMbAYwI2w2mNl7WxnTsM7bjjl9Hh3p82inz6Kj/vB57Nndih3iZLG7zwRmbut2zGx+dz+xjiN9Hh3p82inz6Kj/v55RHlo6BNgZF57RNjX5RgzKwUGAqsjjElERDqJshC8AuxjZqPNrAw4A5jTacwc4B/D5VOAZ1y/ChMR6VORHRoKj/lfCMwFEsA97v62mV0NzHf3OcDdwO/M7ENgDUGxiNI2H17qZ/R5dKTPo50+i4769eexw01DLSIivatfzDUkIiJbT4VARCTmYlMItjTdRVyY2Ugze9bMFpnZ22Z2cbFj2h6YWcLMXjOz7m/wHBNmNsjMHjazd83sHTP7SrFjKhYzuzT8d/KWmdWZWb+cWjcWhaDA6S7iIgNc5u77A5OA78X4s8h3MfBOsYPYTvwaeMLd9wPGEdPPxcyGA98HJrj7gQQXvUR9QUtRxKIQUNh0F7Hg7svd/dVweQPBP/LhxY2quMxsBHAScFexYyk2MxsIHEVwRR/unnL3L4obVVGVApXh75yqgE+LHE8k4lIIupruItbJDyCc7fVQ4C/FjaTobgb+F5ArdiDbgdHASuDe8FDZXWZWXeygisYN+jAAAAQISURBVMHdPwGuB/4KLAfWuft/FzeqaMSlEEgnZlYD/AdwibuvL3Y8xWJmU4AV7r6g2LFsJ0qBvwFuc/dDgY1ALM+pmdlggiMHo4HdgWozO6u4UUUjLoWgkOkuYsPMkgRFYJa7/2ex4ymyI4CpZraU4JDhcWb278UNqajqgXp3b/2W+DBBYYijrwJL3H2lu6eB/wQOL3JMkYhLIShkuotYCKf5vht4x91vLHY8xebuP3L3Ee4+iuD/i2fcvV/u9RXC3T8DlpnZvmHXZGBREUMqpr8Ck8ysKvx3M5l+euJ8h5h9dFt1N91FkcMqliOAfwDeNLPXw74r3P2xIsYk25eLgFnhTtNi4Jwix1MU7v4XM3sYeJXgarvX6KdTTWiKCRGRmIvLoSEREemGCoGISMypEIiIxJwKgYhIzKkQiIjEnAqB7NDMLGtmr+c9eu1XsGY2yszeKmDcVWbWaGY75/U19GUMItsiFr8jkH6tyd0PKXYQwCrgMuBfih1IPjMrdfdMseOQ7Zu+EUi/ZGZLzew6M3vTzF42sy+F/aPM7BkzW2hmT5vZHmH/Lmb2iJm9ET5apxJImNmd4Zz0/21mld285T3A6WY2pFMcHfbozeyHZnZVuPycmd1kZvPDef+/bGb/aWYfmNk1eZspNbNZ4ZiHzawqfP14M3vezBaY2Vwz2y1vuzeb2XyC6bVFNkuFQHZ0lZ0ODZ2et26dux8E/BvBDKMAvwF+6+4HA7OAW8L+W4Dn3X0cwdw6rb883we41d0PAL4ATu4mjgaCYtDTxJty9wnA7cD/Bb4HHAicbWZDwzH7Av/H3ccC64F/CueL+g1wiruPD9/72rztlrn7BHe/oYfxSAzp0JDs6DZ3aKgu7/mmcPkrwDfD5d8B14XLxwHfAnD3LLAunH1yibu3TsWxABi1mVhuAV43s+t7EH/rnFdvAm+7+3IAM1tMMFHiF8Ayd/9zOO7fCW6W8gRBwXgymAaHBMFUya0e6EEMEnMqBNKfeTfLPdGSt5wFujs0hLt/YWa/J9irb5Wh4zfvzrc6bN1+rtN75Wj/99k5dgeMoHB0dxvJjd3FKdKZDg1Jf3Z63vOL4fILtN9u8H8CfwyXnwYugLb7Fw/cyve8EfgO7Un8c2BnMxtqZuXAlK3Y5h559w0+E/gT8B5Q29pvZkkzO2ArY5aYUyGQHV3ncwS/yFs32MwWEhy3vzTsuwg4J+z/B9qP6V8MHGtmbxIcAtqq+zi7+yrgEaA8bKeBq4GXgSeBd7dis+8R3Fv6HWAwwU1jUsApwC/N7A3gdfrpXPkSPc0+Kv1SeKOZCWFiFpHN0DcCEZGY0zcCEZGY0zcCEZGYUyEQEYk5FQIRkZhTIRARiTkVAhGRmPv/R/5e3kxmf1EAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Zjsjxq8zuq"
      },
      "source": [
        "c) Replace your defined CNN in b) with a pre-trained model. Then, proceed with a transfer learning and finetune the model for the Fashion MNIST dataset. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4joDd5u8zur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "9a5fad42effe4dea8be7f95341e2ea87",
            "bc56b855c3b848a99b348b33dde8cca1",
            "6320fef33ae346ba86227989a0296c05",
            "07e6f4e38c844f8695a882bf1ed27f3f",
            "6327597360864735b007bbfca2e722e2",
            "76e8d2cf953b4bf6af5a3b92e7e60b91",
            "059e298f22fb4d2bbb6898378c21214e",
            "99f4bbb5ccf14623a8a6ed6fc12ee51d",
            "7cd8dee0029a4641a4dbe523af2b0bfc",
            "de36f4c4878f4081806130f2b7bb5f1f",
            "4a5e3430ff7048caa9358a2920651e2c"
          ]
        },
        "outputId": "e27b57fb-6c4e-48f0-edc2-5ef8d811e132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a5fad42effe4dea8be7f95341e2ea87"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "###############################################\n",
        "###############YOUR CODES HERE ################\n",
        "###############################################\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# In case you wanna experiment with fashinMNIST dataset\n",
        "\n",
        "transform = transforms.Compose(\n",
        "     [transforms.Resize(32),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x:x.repeat(3,1,1) \n",
        "      if x.size(0)==1 else x),\n",
        "      transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "\n",
        "\n",
        "# # batch_size\n",
        "batch_size = 4\n",
        "\n",
        "# # datasets\n",
        "trainset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=True,\n",
        "     transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=False,\n",
        "     transform=transform)\n",
        "\n",
        "# # dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# # constant for classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "#pretrained model\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "#define new classification layer\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lufHPW1amuPO",
        "outputId": "0be7e38a-8ad1-4598-e9c0-5a7ae8cb7b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer_ft, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDEySyZ6cw5T",
        "outputId": "c790dc2c-7b56-4d93-f55d-88da92eb47e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.9416, Accuracy: 70.5333%, \n",
            "\t\tValidation : Loss : 0.6388, Accuracy: 78.1200%, Time: 225.2840s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.5393, Accuracy: 82.8767%, \n",
            "\t\tValidation : Loss : 0.5176, Accuracy: 86.8000%, Time: 217.8037s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.4506, Accuracy: 85.6133%, \n",
            "\t\tValidation : Loss : 0.4510, Accuracy: 86.7500%, Time: 215.9204s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3771, Accuracy: 87.8483%, \n",
            "\t\tValidation : Loss : 0.4408, Accuracy: 85.8100%, Time: 215.9537s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.3632, Accuracy: 88.3250%, \n",
            "\t\tValidation : Loss : 0.6858, Accuracy: 79.4900%, Time: 215.9696s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.4004, Accuracy: 87.2650%, \n",
            "\t\tValidation : Loss : 0.4048, Accuracy: 87.8700%, Time: 215.6307s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.3197, Accuracy: 89.3600%, \n",
            "\t\tValidation : Loss : 0.3913, Accuracy: 89.6300%, Time: 215.3122s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2918, Accuracy: 90.3033%, \n",
            "\t\tValidation : Loss : 0.3476, Accuracy: 90.4800%, Time: 222.7955s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2808, Accuracy: 90.7317%, \n",
            "\t\tValidation : Loss : 0.3539, Accuracy: 89.3500%, Time: 215.8550s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2706, Accuracy: 91.0867%, \n",
            "\t\tValidation : Loss : 0.3429, Accuracy: 89.8000%, Time: 215.3889s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uCpzFC8zur"
      },
      "source": [
        "d) Using model-centric methods, propose two (2) strategies that can be used to increase the accuracy of the model on the testing dataset. **[5 marks]**\n",
        "\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Two model-centric techniques that I propose are: Batchnorm and Dropout </span> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FIMfUfz8zur"
      },
      "source": [
        "e) Next, implement the two proposed model-centric techniques for the same problem as in the previous question. **[15 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UIGCk5K8zus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34085ed8-b892-49ee-b811-dbf0050abe19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "###############################################\n",
        "###############YOUR CODES HERE ################\n",
        "###############################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "     [transforms.Resize(32),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "\n",
        "# # batch_size\n",
        "batch_size = 4\n",
        "\n",
        "# # datasets\n",
        "trainset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=True,\n",
        "     transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=False,\n",
        "     transform=transform)\n",
        "\n",
        "# # dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# # constant for classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)\n",
        "\n",
        "#1. DEFINE THE CNN \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        #(input channel, output channel, kernel size)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        #(kernel size, stride)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.batchnorm = nn.BatchNorm2d(16)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.dropout = nn.Dropout(p = 0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.batchnorm(x)\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN() # need to instantiate the network to be used in instance method\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHF9XIXQhn0-",
        "outputId": "493c4787-5185-422a-a543-efc298e64103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.4645, Accuracy: 83.1133%, \n",
            "\t\tValidation : Loss : 0.3605, Accuracy: 86.2900%, Time: 72.3802s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.3363, Accuracy: 87.6233%, \n",
            "\t\tValidation : Loss : 0.3334, Accuracy: 87.5900%, Time: 72.9316s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3025, Accuracy: 88.8617%, \n",
            "\t\tValidation : Loss : 0.3124, Accuracy: 88.5600%, Time: 73.7067s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2811, Accuracy: 89.6367%, \n",
            "\t\tValidation : Loss : 0.2950, Accuracy: 89.3600%, Time: 73.2501s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2652, Accuracy: 90.1817%, \n",
            "\t\tValidation : Loss : 0.2858, Accuracy: 89.3800%, Time: 72.7809s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2517, Accuracy: 90.7417%, \n",
            "\t\tValidation : Loss : 0.2847, Accuracy: 89.8200%, Time: 73.6002s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.2384, Accuracy: 91.1717%, \n",
            "\t\tValidation : Loss : 0.2786, Accuracy: 90.0600%, Time: 73.4474s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2301, Accuracy: 91.5433%, \n",
            "\t\tValidation : Loss : 0.2871, Accuracy: 89.7400%, Time: 72.9193s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2198, Accuracy: 91.8150%, \n",
            "\t\tValidation : Loss : 0.2724, Accuracy: 90.2300%, Time: 72.9648s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2114, Accuracy: 91.9083%, \n",
            "\t\tValidation : Loss : 0.2759, Accuracy: 90.1900%, Time: 73.0538s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPPxsCX8zus"
      },
      "source": [
        "f) Do you see any accuracy improvement? Whether it is a \"yes\" or \"no\", discuss the possible reasons contributing to the accuracy improvement/ unimprovement. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    There is 0.85% differences between CNN and Pre-trained, there is 0.6% differences between CNN and Model Centric. Hence, we can see an improvement for both of the method. Using a batch normalization will reduces the dependence of the network to the weight initialization and improves the gradient flow throgh the network. \n",
        "    Using a pretrained model is significantly more accurate than using a custom built CNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVArqW8h8zus"
      },
      "source": [
        "g) In real applications, data-centric strategies are essential to train robust deep learning models. Give two (2) examples of such strategies and discuss how the strategies helps improving the model accuracy. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Using data augmentation will increase the size of the training set but also in avoiding overfitting. Secondly, using data mixup will generates a weighted combinations of random images pairs from the training data. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zifLt-s8zut"
      },
      "source": [
        "h) Next, implement the two proposed data-centric techniques for the same problem as in the previous question. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "     [transforms.Resize(32),\n",
        "      transforms.RandomRotation(degrees=15),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x:x.repeat(3,1,1) \n",
        "      if x.size(0)==1 else x),\n",
        "      transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "\n",
        "# # batch_size\n",
        "batch_size = 4\n",
        "\n",
        "# # datasets\n",
        "trainset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=True,\n",
        "     transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST('./data',\n",
        "     download=True,\n",
        "     train=False,\n",
        "     transform=transform)\n",
        "\n",
        "# # dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# # constant for classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYF_RudhICIs",
        "outputId": "a372d15c-1eaf-4943-96ba-1e6e8901f1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. DEFINE THE CNN \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5) \n",
        "        #(input channel, output channel, kernel size)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        #(kernel size, stride)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "n_gacfHSIRiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_ft.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t-GVi7fIUPC",
        "outputId": "572503c4-7f29-4fe8-ae01-0290c89b3b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEVE8du9pGPz"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "        \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlpOWa1cIsN4",
        "outputId": "704de541-2e1a-4364-dda7-22908e8f4449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.5135, Accuracy: 83.9933%, \n",
            "\t\tValidation : Loss : 0.4429, Accuracy: 87.3000%, Time: 228.2486s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.4096, Accuracy: 86.6917%, \n",
            "\t\tValidation : Loss : 0.3939, Accuracy: 88.1900%, Time: 226.7297s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3952, Accuracy: 86.8567%, \n",
            "\t\tValidation : Loss : 0.4407, Accuracy: 86.9800%, Time: 243.1183s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3548, Accuracy: 88.2483%, \n",
            "\t\tValidation : Loss : 0.3951, Accuracy: 88.1300%, Time: 231.5181s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.3438, Accuracy: 88.7333%, \n",
            "\t\tValidation : Loss : 0.3524, Accuracy: 88.3400%, Time: 231.7671s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.3327, Accuracy: 88.7950%, \n",
            "\t\tValidation : Loss : 0.4350, Accuracy: 86.7900%, Time: 239.7120s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.3212, Accuracy: 89.0483%, \n",
            "\t\tValidation : Loss : 0.4072, Accuracy: 87.5700%, Time: 231.1685s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.3160, Accuracy: 89.4267%, \n",
            "\t\tValidation : Loss : 0.3519, Accuracy: 88.9800%, Time: 223.7112s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.3063, Accuracy: 89.6433%, \n",
            "\t\tValidation : Loss : 0.3396, Accuracy: 89.4600%, Time: 229.8288s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2999, Accuracy: 89.7400%, \n",
            "\t\tValidation : Loss : 0.3225, Accuracy: 89.5600%, Time: 221.6153s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCy3b5888zut"
      },
      "source": [
        "**QUESTION 2** **[35 marks]**\n",
        "\n",
        "Firstly, watch this video:\n",
        "\n",
        "https://drive.google.com/file/d/1bsypahR7I3f_R3DXkfw_tf0BrbCHxE_O/view?usp=sharing\n",
        "\n",
        "This video shows an example of masked face recognition where the deep learning model is able to detect and classify your face even when wearing a face mask. Using the end-to-end object detection pipeline that you have learned, develop your own masked face recognition such that the model should recognize your face even on face mask while recognize other persons as \"others\".\n",
        "\n",
        "Deliverables for this question are:\n",
        "\n",
        "- the model file. Change the name to <your_name>.pt file (e.g. hasan.pt).\n",
        "- a short video (~10 secs) containing your face and your friends faces (for inference)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Link : https://drive.google.com/file/d/1ewpu5Hk-MQ-qoo0ef6o-ePqw5OFfTEMY/view?usp=sharing\n",
        "\n",
        "Fitri.pt Link : https://drive.google.com/file/d/114wcCuoOBecPieC6zj-FFrlWwFDHanWW/view?usp=sharing"
      ],
      "metadata": {
        "id": "S9qScQG7MEV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-2fGUgMvfJl",
        "outputId": "d96cdaab-593f-4555-edd1-f51de31d8e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 13478, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 13478 (delta 201), reused 250 (delta 180), pack-reused 13198\u001b[K\n",
            "Receiving objects: 100% (13478/13478), 13.18 MiB | 17.85 MiB/s, done.\n",
            "Resolving deltas: 100% (9262/9262), done.\n",
            "/content/yolov5/yolov5\n",
            "Setup complete. Using torch 1.12.1+cu113 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ],
      "metadata": {
        "id": "6eyJ6cqtvoba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c325d52b-70a7-4a19-9519-4af8a99f21ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ],
      "metadata": {
        "id": "88djQWTivpTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after following the link above, recieve python code with these fields filled in\n",
        "#from roboflow import Roboflow\n",
        "#rf = Roboflow(api_key=\"YOUR API KEY HERE\")\n",
        "#project = rf.workspace().project(\"YOUR PROJECT\")\n",
        "#dataset = project.version(\"YOUR VERSION\").download(\"yolov5\")\n",
        "#copy from roboflow\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"D6OHFSqWEYUNXKUZ8Sk1\")\n",
        "project = rf.workspace(\"deep-learning-workshop-dhlre\").project(\"fitdanothers\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMnUjvgwvrz1",
        "outputId": "ef9f1679-8ee5-4a6b-b1b0-ffd01a151c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.7/dist-packages (0.2.15)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.3.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.21.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.1)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in /content/datasets/FITDANOTHERS-1 to yolov5pytorch: 100% [3428045 / 3428045] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/FITDANOTHERS-1 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:00<00:00, 1905.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 150 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache\n",
        "#weights can change, go back to ultralytics. n-nano, s-small, m-medium, l-large, x-xlarge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGFU9DqOv7_r",
        "outputId": "cfb01994-3a26-4e1e-81e6-56445f130ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/datasets/FITDANOTHERS-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=150, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.2-187-g5ef69ef Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 32.7MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 328MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/FITDANOTHERS-1/train/labels' images and labels...158 found, 0 missing, 0 empty, 0 corrupt: 100% 158/158 [00:00<00:00, 2027.89it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/FITDANOTHERS-1/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 158/158 [00:00<00:00, 438.78it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/FITDANOTHERS-1/valid/labels' images and labels...9 found, 0 missing, 0 empty, 0 corrupt: 100% 9/9 [00:00<00:00, 395.50it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/FITDANOTHERS-1/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 9/9 [00:00<00:00, 190.63it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.82 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 150 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      0/149      1.71G     0.1186    0.02661    0.03017         52        416: 100% 10/10 [00:05<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.27s/it]\n",
            "                   all          9         18    0.00332      0.512     0.0067    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      1/149      1.71G     0.1018    0.03457    0.02744         61        416: 100% 10/10 [00:01<00:00,  6.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.01it/s]\n",
            "                   all          9         18     0.0045        0.7     0.0153    0.00294\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      2/149      1.71G    0.08822    0.03776    0.02411         84        416: 100% 10/10 [00:01<00:00,  6.28it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  8.83it/s]\n",
            "                   all          9         18     0.0446      0.738     0.0759     0.0322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      3/149      1.71G    0.07791    0.03949    0.02156         56        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.24it/s]\n",
            "                   all          9         18      0.162      0.625      0.216     0.0804\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      4/149      1.71G     0.0712    0.03682    0.02145         51        416: 100% 10/10 [00:01<00:00,  6.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.80it/s]\n",
            "                   all          9         18      0.234      0.666      0.333     0.0918\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      5/149      1.71G    0.06774    0.03562    0.01869         49        416: 100% 10/10 [00:01<00:00,  6.31it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.46it/s]\n",
            "                   all          9         18      0.169      0.487      0.195     0.0741\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      6/149      1.71G    0.06507    0.03397    0.01767         80        416: 100% 10/10 [00:01<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.99it/s]\n",
            "                   all          9         18      0.245      0.425      0.245     0.0821\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      7/149      1.71G    0.06391    0.03165    0.01786         52        416: 100% 10/10 [00:01<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.04it/s]\n",
            "                   all          9         18      0.177      0.762      0.244     0.0804\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      8/149      1.71G    0.06795    0.03001    0.01777         59        416: 100% 10/10 [00:01<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.99it/s]\n",
            "                   all          9         18      0.139        0.7      0.222     0.0789\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      9/149      1.71G    0.06903    0.03107     0.0173         55        416: 100% 10/10 [00:01<00:00,  6.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.98it/s]\n",
            "                   all          9         18      0.227      0.287       0.17     0.0343\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     10/149      1.71G    0.06257    0.02886    0.01798         50        416: 100% 10/10 [00:01<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.15it/s]\n",
            "                   all          9         18      0.137      0.537      0.138     0.0398\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     11/149      1.71G    0.06039    0.02613    0.01832         51        416: 100% 10/10 [00:01<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.07it/s]\n",
            "                   all          9         18      0.349      0.681      0.438      0.126\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     12/149      1.71G    0.05714    0.02987    0.01721         77        416: 100% 10/10 [00:01<00:00,  6.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.95it/s]\n",
            "                   all          9         18      0.296      0.688      0.386      0.118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     13/149      1.71G    0.05716    0.02929    0.01658         62        416: 100% 10/10 [00:01<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.47it/s]\n",
            "                   all          9         18      0.281      0.811      0.328      0.119\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     14/149      1.71G    0.05177    0.02619    0.01625         66        416: 100% 10/10 [00:01<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.16it/s]\n",
            "                   all          9         18       0.32        0.6      0.354      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     15/149      1.71G    0.05678    0.02426    0.01494         46        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.94it/s]\n",
            "                   all          9         18      0.393      0.662      0.416      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     16/149      1.71G    0.04954    0.02642    0.01414         44        416: 100% 10/10 [00:01<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.64it/s]\n",
            "                   all          9         18      0.333      0.613      0.347     0.0938\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     17/149      1.71G    0.04766    0.02606    0.01348         60        416: 100% 10/10 [00:01<00:00,  6.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.30it/s]\n",
            "                   all          9         18      0.633       0.75      0.772      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     18/149      1.71G    0.05012    0.02643    0.01127         55        416: 100% 10/10 [00:01<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.93it/s]\n",
            "                   all          9         18      0.698        0.8       0.78      0.244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     19/149      1.71G    0.04809    0.02566    0.01148         76        416: 100% 10/10 [00:01<00:00,  6.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.97it/s]\n",
            "                   all          9         18       0.82      0.762      0.843      0.278\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     20/149      1.71G    0.04803    0.02491    0.00939         61        416: 100% 10/10 [00:01<00:00,  6.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.59it/s]\n",
            "                   all          9         18      0.729      0.743      0.747      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     21/149      1.71G    0.04528    0.02296   0.009713         44        416: 100% 10/10 [00:01<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.24it/s]\n",
            "                   all          9         18      0.545       0.72      0.659       0.15\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     22/149      1.71G    0.04683     0.0235   0.008409         54        416: 100% 10/10 [00:01<00:00,  6.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.11it/s]\n",
            "                   all          9         18      0.806      0.777      0.797      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     23/149      1.71G    0.04797    0.02518   0.007262         59        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.18it/s]\n",
            "                   all          9         18      0.809      0.775      0.862      0.396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     24/149      1.71G    0.04517    0.02366   0.007164         42        416: 100% 10/10 [00:01<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.21it/s]\n",
            "                   all          9         18      0.746      0.863      0.899      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     25/149      1.71G    0.04287    0.02355   0.006557         76        416: 100% 10/10 [00:01<00:00,  6.23it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.14it/s]\n",
            "                   all          9         18      0.635      0.889      0.709      0.247\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     26/149      1.71G    0.04187    0.02272    0.00604         52        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.80it/s]\n",
            "                   all          9         18      0.816      0.935      0.902       0.28\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     27/149      1.71G     0.0435    0.02206   0.006545         57        416: 100% 10/10 [00:01<00:00,  6.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.22it/s]\n",
            "                   all          9         18      0.859      0.879      0.848      0.262\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     28/149      1.71G     0.0423    0.02227   0.004931         53        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.66it/s]\n",
            "                   all          9         18      0.701      0.973      0.887      0.268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     29/149      1.71G    0.04182    0.02293   0.004255         52        416: 100% 10/10 [00:01<00:00,  6.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.00it/s]\n",
            "                   all          9         18      0.723      0.875      0.869      0.299\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     30/149      1.71G    0.03995    0.02207   0.003944         52        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.50it/s]\n",
            "                   all          9         18      0.673      0.756      0.806      0.272\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     31/149      1.71G    0.04148    0.02075   0.004124         70        416: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.87it/s]\n",
            "                   all          9         18       0.94      0.897      0.953      0.446\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     32/149      1.71G    0.04137    0.02146   0.004056         59        416: 100% 10/10 [00:01<00:00,  6.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.18it/s]\n",
            "                   all          9         18      0.857      0.937      0.942      0.315\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     33/149      1.71G    0.04246    0.02116   0.003811         70        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.32it/s]\n",
            "                   all          9         18      0.904       0.95       0.91       0.26\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     34/149      1.71G    0.03912    0.02327   0.004027         68        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.02it/s]\n",
            "                   all          9         18      0.882      0.938        0.9      0.258\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     35/149      1.71G    0.04008    0.02197   0.003682         76        416: 100% 10/10 [00:01<00:00,  6.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.90it/s]\n",
            "                   all          9         18      0.977      0.913      0.983      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     36/149      1.71G    0.03804    0.02153   0.003576         56        416: 100% 10/10 [00:01<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.33it/s]\n",
            "                   all          9         18      0.722      0.794      0.719      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     37/149      1.71G    0.03986    0.02185   0.003352         89        416: 100% 10/10 [00:01<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.24it/s]\n",
            "                   all          9         18      0.865      0.821      0.843      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     38/149      1.71G    0.03546    0.02054   0.003513         56        416: 100% 10/10 [00:01<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.13it/s]\n",
            "                   all          9         18      0.873      0.817      0.823      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     39/149      1.71G    0.03747    0.02164   0.003008         60        416: 100% 10/10 [00:01<00:00,  6.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.31it/s]\n",
            "                   all          9         18      0.949      0.918      0.959      0.454\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     40/149      1.71G    0.03542    0.02081   0.003208         37        416: 100% 10/10 [00:01<00:00,  6.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.74it/s]\n",
            "                   all          9         18      0.947       0.96       0.99      0.484\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     41/149      1.71G     0.0362    0.02087    0.00438         80        416: 100% 10/10 [00:01<00:00,  6.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.08it/s]\n",
            "                   all          9         18          1      0.937      0.995      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     42/149      1.71G    0.03527    0.02156    0.00282         64        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.31it/s]\n",
            "                   all          9         18      0.979      0.938      0.969      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     43/149      1.71G    0.03462    0.02157   0.002398         64        416: 100% 10/10 [00:01<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.10it/s]\n",
            "                   all          9         18      0.986      0.944      0.995      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     44/149      1.71G    0.03352    0.01924   0.003818         60        416: 100% 10/10 [00:01<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.82it/s]\n",
            "                   all          9         18      0.987      0.938      0.979      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     45/149      1.71G    0.03443    0.02004   0.003378         72        416: 100% 10/10 [00:01<00:00,  6.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.15it/s]\n",
            "                   all          9         18          1      0.931      0.963      0.477\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     46/149      1.71G    0.03513    0.02176   0.003228         69        416: 100% 10/10 [00:01<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.87it/s]\n",
            "                   all          9         18      0.992      0.938      0.955      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     47/149      1.71G    0.03295    0.02252   0.003112         46        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.42it/s]\n",
            "                   all          9         18      0.939      0.887      0.896      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     48/149      1.71G    0.03419    0.01839   0.002424         60        416: 100% 10/10 [00:01<00:00,  6.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.98it/s]\n",
            "                   all          9         18      0.939      0.887      0.881      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     49/149      1.71G    0.03475    0.01926   0.002813         55        416: 100% 10/10 [00:01<00:00,  6.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.49it/s]\n",
            "                   all          9         18      0.991      0.935      0.969      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     50/149      1.71G     0.0329    0.01883    0.00244         60        416: 100% 10/10 [00:01<00:00,  6.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.23it/s]\n",
            "                   all          9         18      0.857      0.918      0.953      0.488\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     51/149      1.71G    0.03686    0.02178   0.002626         55        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.23it/s]\n",
            "                   all          9         18      0.939      0.938      0.961      0.424\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     52/149      1.71G    0.03284    0.01982   0.002498         59        416: 100% 10/10 [00:01<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.47it/s]\n",
            "                   all          9         18      0.921      0.987      0.986      0.445\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     53/149      1.71G    0.03309    0.01924   0.002532         48        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.02it/s]\n",
            "                   all          9         18      0.984      0.938      0.988      0.394\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     54/149      1.71G    0.03199    0.01814   0.002852         45        416: 100% 10/10 [00:01<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.86it/s]\n",
            "                   all          9         18      0.899      0.875      0.882      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     55/149      1.71G    0.02966     0.0201   0.002303         56        416: 100% 10/10 [00:01<00:00,  5.34it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.98it/s]\n",
            "                   all          9         18      0.878      0.875      0.869      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     56/149      1.71G    0.03329    0.01931   0.002491         38        416: 100% 10/10 [00:02<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.50it/s]\n",
            "                   all          9         18      0.915      0.875      0.869      0.329\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     57/149      1.71G    0.03168    0.01834   0.002675         53        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.20it/s]\n",
            "                   all          9         18      0.978      0.919      0.962      0.433\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     58/149      1.71G    0.03298    0.01858   0.002201         46        416: 100% 10/10 [00:01<00:00,  6.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.64it/s]\n",
            "                   all          9         18      0.931      0.884      0.867      0.359\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     59/149      1.71G    0.03163    0.01859   0.002489         38        416: 100% 10/10 [00:01<00:00,  6.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.19it/s]\n",
            "                   all          9         18       0.92      0.871      0.962      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     60/149      1.71G    0.03071    0.01784   0.001905         51        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.38it/s]\n",
            "                   all          9         18      0.858       0.92      0.956      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     61/149      1.71G    0.02949    0.01996   0.001803         65        416: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.55it/s]\n",
            "                   all          9         18      0.811      0.883      0.846      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     62/149      1.71G    0.03035    0.01919   0.002247         61        416: 100% 10/10 [00:01<00:00,  6.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.54it/s]\n",
            "                   all          9         18       0.92      0.985      0.995      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     63/149      1.71G    0.03104    0.01914   0.001791         51        416: 100% 10/10 [00:01<00:00,  6.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.00it/s]\n",
            "                   all          9         18      0.921      0.973      0.986      0.364\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     64/149      1.71G    0.02909    0.01788    0.00212         55        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.62it/s]\n",
            "                   all          9         18      0.921       0.99      0.986      0.435\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     65/149      1.71G     0.0301    0.01807   0.001751         59        416: 100% 10/10 [00:01<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.15it/s]\n",
            "                   all          9         18      0.986          1      0.995      0.554\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     66/149      1.71G    0.03008    0.01979   0.002004         76        416: 100% 10/10 [00:01<00:00,  6.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.62it/s]\n",
            "                   all          9         18      0.984          1      0.995       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     67/149      1.71G    0.03052    0.01874   0.002503         77        416: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.85it/s]\n",
            "                   all          9         18      0.989          1      0.995      0.488\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     68/149      1.71G    0.03013    0.01708   0.001485         55        416: 100% 10/10 [00:01<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.09it/s]\n",
            "                   all          9         18      0.991          1      0.995      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     69/149      1.71G     0.0282    0.01844   0.001735         51        416: 100% 10/10 [00:01<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.40it/s]\n",
            "                   all          9         18      0.992      0.982      0.995      0.533\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     70/149      1.71G    0.02977    0.02007     0.0016         60        416: 100% 10/10 [00:01<00:00,  6.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.04it/s]\n",
            "                   all          9         18      0.993      0.996      0.995      0.591\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     71/149      1.71G    0.02801    0.01838   0.001572         56        416: 100% 10/10 [00:01<00:00,  6.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.42it/s]\n",
            "                   all          9         18      0.921      0.981       0.99       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     72/149      1.71G    0.02887    0.01669   0.001555         67        416: 100% 10/10 [00:01<00:00,  6.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.10it/s]\n",
            "                   all          9         18       0.95      0.949      0.981      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     73/149      1.71G    0.02791    0.01663    0.00195         75        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.90it/s]\n",
            "                   all          9         18      0.932      0.938      0.986      0.466\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     74/149      1.71G    0.02829    0.01894   0.001251         49        416: 100% 10/10 [00:01<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.15it/s]\n",
            "                   all          9         18      0.921      0.976      0.986      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     75/149      1.71G    0.02993     0.0184   0.001475         73        416: 100% 10/10 [00:01<00:00,  6.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.36it/s]\n",
            "                   all          9         18      0.919      0.983      0.995      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     76/149      1.71G    0.02742    0.01806    0.00142         68        416: 100% 10/10 [00:01<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.98it/s]\n",
            "                   all          9         18       0.92      0.984       0.99      0.468\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     77/149      1.71G    0.02776    0.01794   0.001489         50        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.14it/s]\n",
            "                   all          9         18      0.855      0.925      0.927      0.356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     78/149      1.71G     0.0281    0.01706   0.001164         64        416: 100% 10/10 [00:01<00:00,  6.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.36it/s]\n",
            "                   all          9         18      0.855      0.924      0.924      0.348\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     79/149      1.71G    0.02489    0.01599   0.001067         64        416: 100% 10/10 [00:01<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.63it/s]\n",
            "                   all          9         18       0.95      0.955      0.986      0.396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     80/149      1.71G     0.0281    0.01694   0.002225         47        416: 100% 10/10 [00:01<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.26it/s]\n",
            "                   all          9         18       0.95       0.96      0.986      0.427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     81/149      1.71G    0.02833    0.01796   0.001982         69        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.65it/s]\n",
            "                   all          9         18      0.999      0.962      0.995      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     82/149      1.71G    0.02649    0.01631   0.001729         49        416: 100% 10/10 [00:01<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.47it/s]\n",
            "                   all          9         18      0.927      0.985       0.99      0.432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     83/149      1.71G    0.02611     0.0175   0.001128         59        416: 100% 10/10 [00:01<00:00,  6.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.57it/s]\n",
            "                   all          9         18      0.857      0.932      0.915      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     84/149      1.71G    0.02716    0.01694    0.00142         80        416: 100% 10/10 [00:01<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.60it/s]\n",
            "                   all          9         18      0.856      0.929      0.914      0.451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     85/149      1.71G    0.02629    0.01763   0.001486         47        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.59it/s]\n",
            "                   all          9         18       0.92      0.988       0.99      0.511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     86/149      1.71G    0.02633    0.01778   0.001205         56        416: 100% 10/10 [00:01<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.38it/s]\n",
            "                   all          9         18       0.92      0.984       0.99      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     87/149      1.71G    0.02648    0.01772   0.001248         70        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.37it/s]\n",
            "                   all          9         18      0.953      0.925      0.975       0.52\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     88/149      1.71G    0.02579      0.017   0.001331         55        416: 100% 10/10 [00:01<00:00,  6.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.11it/s]\n",
            "                   all          9         18      0.944      0.938      0.969      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     89/149      1.71G    0.02481    0.01731   0.001708         65        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.51it/s]\n",
            "                   all          9         18      0.937      0.942      0.995      0.567\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     90/149      1.71G    0.02708     0.0169   0.001203         61        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.09it/s]\n",
            "                   all          9         18      0.974      0.938      0.995      0.628\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     91/149      1.71G    0.02543    0.01762   0.001259         63        416: 100% 10/10 [00:01<00:00,  6.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.40it/s]\n",
            "                   all          9         18      0.975      0.938      0.988      0.564\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     92/149      1.71G    0.02576    0.01716   0.001075         64        416: 100% 10/10 [00:01<00:00,  6.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.68it/s]\n",
            "                   all          9         18      0.971      0.941      0.995      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     93/149      1.71G     0.0259     0.0169   0.001206         62        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.40it/s]\n",
            "                   all          9         18      0.992      0.952      0.995      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     94/149      1.71G    0.02599    0.01692    0.00159         58        416: 100% 10/10 [00:01<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.45it/s]\n",
            "                   all          9         18       0.98      0.991      0.995      0.528\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     95/149      1.71G    0.02638    0.01708   0.001066         70        416: 100% 10/10 [00:01<00:00,  6.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.25it/s]\n",
            "                   all          9         18       0.98      0.996      0.995      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     96/149      1.71G    0.02535    0.01637   0.001018         70        416: 100% 10/10 [00:01<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.87it/s]\n",
            "                   all          9         18      0.973      0.995      0.995      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     97/149      1.71G    0.02463    0.01676    0.00145         69        416: 100% 10/10 [00:01<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.79it/s]\n",
            "                   all          9         18      0.991          1      0.995      0.472\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     98/149      1.71G    0.02408    0.01763  0.0008226         74        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.92it/s]\n",
            "                   all          9         18      0.989          1      0.995      0.494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     99/149      1.71G    0.02556    0.01648  0.0009573         46        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.45it/s]\n",
            "                   all          9         18      0.979      0.996      0.995      0.491\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    100/149      1.71G    0.02374    0.01607   0.001001         37        416: 100% 10/10 [00:01<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.30it/s]\n",
            "                   all          9         18      0.991          1      0.995      0.467\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    101/149      1.71G    0.02516    0.01653   0.001673         53        416: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.97it/s]\n",
            "                   all          9         18      0.956      0.992      0.995      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    102/149      1.71G    0.02442    0.01681  0.0008609         38        416: 100% 10/10 [00:01<00:00,  6.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.02it/s]\n",
            "                   all          9         18      0.974      0.996      0.995      0.463\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    103/149      1.71G    0.02458    0.01704  0.0009809         70        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.94it/s]\n",
            "                   all          9         18       0.99      0.946      0.995      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    104/149      1.71G    0.02468    0.01669   0.001128         62        416: 100% 10/10 [00:01<00:00,  6.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.97it/s]\n",
            "                   all          9         18       0.99      0.953      0.995      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    105/149      1.71G    0.02568    0.01565  0.0009972         47        416: 100% 10/10 [00:01<00:00,  6.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.16it/s]\n",
            "                   all          9         18       0.92      0.986       0.99       0.49\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    106/149      1.71G     0.0236     0.0158  0.0009055         59        416: 100% 10/10 [00:01<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.33it/s]\n",
            "                   all          9         18       0.92      0.981       0.99      0.513\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    107/149      1.71G    0.02364    0.01667   0.001338         40        416: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.86it/s]\n",
            "                   all          9         18      0.956       0.94      0.995      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    108/149      1.71G    0.02244    0.01611   0.001097         80        416: 100% 10/10 [00:01<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.93it/s]\n",
            "                   all          9         18      0.985      0.946      0.995       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    109/149      1.71G    0.02157    0.01486  0.0009554         54        416: 100% 10/10 [00:01<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.64it/s]\n",
            "                   all          9         18      0.963      0.993      0.995      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    110/149      1.71G    0.02285      0.017  0.0009308         81        416: 100% 10/10 [00:01<00:00,  6.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.12it/s]\n",
            "                   all          9         18       0.98      0.998      0.995      0.541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    111/149      1.71G     0.0224    0.01527  0.0008936         55        416: 100% 10/10 [00:01<00:00,  6.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.56it/s]\n",
            "                   all          9         18      0.983          1      0.995      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    112/149      1.71G    0.02228    0.01601  0.0009116         66        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.26it/s]\n",
            "                   all          9         18      0.989       0.96      0.995      0.557\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    113/149      1.71G     0.0222    0.01483   0.001674         54        416: 100% 10/10 [00:01<00:00,  6.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.79it/s]\n",
            "                   all          9         18      0.986      0.951      0.995      0.585\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    114/149      1.71G     0.0227    0.01514  0.0009173         58        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.73it/s]\n",
            "                   all          9         18      0.988      0.952      0.995      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    115/149      1.71G      0.023    0.01619  0.0006627         49        416: 100% 10/10 [00:01<00:00,  6.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.97it/s]\n",
            "                   all          9         18      0.991      0.963      0.995      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    116/149      1.71G    0.02227    0.01538   0.001752         53        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.56it/s]\n",
            "                   all          9         18       0.99      0.991      0.995      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    117/149      1.71G    0.02203    0.01597  0.0008036         51        416: 100% 10/10 [00:01<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.60it/s]\n",
            "                   all          9         18      0.991      0.966      0.995      0.531\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    118/149      1.71G    0.02301    0.01676   0.001191         59        416: 100% 10/10 [00:01<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.49it/s]\n",
            "                   all          9         18      0.991      0.978      0.995      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    119/149      1.71G    0.02267    0.01577   0.001587         51        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.76it/s]\n",
            "                   all          9         18      0.972      0.955      0.995      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    120/149      1.71G    0.02263    0.01616  0.0008445         73        416: 100% 10/10 [00:01<00:00,  6.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.74it/s]\n",
            "                   all          9         18       0.92      0.992      0.995      0.566\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    121/149      1.71G    0.02223    0.01654  0.0007449         49        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.55it/s]\n",
            "                   all          9         18      0.982      0.954      0.995      0.609\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    122/149      1.71G    0.02136    0.01612   0.001378         72        416: 100% 10/10 [00:01<00:00,  6.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.46it/s]\n",
            "                   all          9         18      0.989      0.958      0.995      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    123/149      1.71G    0.02112    0.01523   0.000799         71        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.11it/s]\n",
            "                   all          9         18      0.991      0.961      0.995      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    124/149      1.71G    0.02229    0.01553   0.001047         67        416: 100% 10/10 [00:01<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.85it/s]\n",
            "                   all          9         18      0.969      0.991      0.995      0.515\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    125/149      1.71G    0.02129    0.01606  0.0006671         71        416: 100% 10/10 [00:01<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.24it/s]\n",
            "                   all          9         18      0.973      0.994      0.995      0.572\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    126/149      1.71G    0.02038    0.01616  0.0008101         57        416: 100% 10/10 [00:01<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.10it/s]\n",
            "                   all          9         18      0.986       0.95      0.995      0.525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    127/149      1.71G    0.02138     0.0158  0.0007533         56        416: 100% 10/10 [00:01<00:00,  6.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.17it/s]\n",
            "                   all          9         18      0.959       0.95      0.995      0.525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    128/149      1.71G    0.02218    0.01538  0.0007508         70        416: 100% 10/10 [00:01<00:00,  6.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.52it/s]\n",
            "                   all          9         18      0.933      0.971      0.986      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    129/149      1.71G    0.02153    0.01533  0.0008367         61        416: 100% 10/10 [00:01<00:00,  6.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.65it/s]\n",
            "                   all          9         18      0.944      0.962       0.99      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    130/149      1.71G     0.0201    0.01485   0.000824         53        416: 100% 10/10 [00:01<00:00,  6.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.45it/s]\n",
            "                   all          9         18          1      0.934      0.995      0.513\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    131/149      1.71G    0.02042    0.01527  0.0005635         86        416: 100% 10/10 [00:01<00:00,  6.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.21it/s]\n",
            "                   all          9         18      0.934      0.975       0.99      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    132/149      1.71G    0.02049     0.0145  0.0008901         64        416: 100% 10/10 [00:01<00:00,  6.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.96it/s]\n",
            "                   all          9         18      0.939      0.968       0.99      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    133/149      1.71G    0.02059    0.01558   0.001036         87        416: 100% 10/10 [00:01<00:00,  6.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.26it/s]\n",
            "                   all          9         18      0.947      0.958       0.99      0.493\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    134/149      1.71G     0.0206    0.01458  0.0006277         52        416: 100% 10/10 [00:01<00:00,  6.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  9.62it/s]\n",
            "                   all          9         18      0.938      0.967       0.99      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    135/149      1.71G    0.02002    0.01459  0.0008553         50        416: 100% 10/10 [00:01<00:00,  6.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.58it/s]\n",
            "                   all          9         18      0.994       0.95      0.995      0.514\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    136/149      1.71G    0.02043    0.01588  0.0006443         74        416: 100% 10/10 [00:01<00:00,  6.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.07it/s]\n",
            "                   all          9         18       0.99      0.951      0.995      0.557\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    137/149      1.71G    0.02036    0.01443  0.0008698         50        416: 100% 10/10 [00:01<00:00,  6.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.70it/s]\n",
            "                   all          9         18      0.989      0.951      0.995      0.532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    138/149      1.71G    0.02057    0.01512   0.001301         70        416: 100% 10/10 [00:01<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.25it/s]\n",
            "                   all          9         18       0.99      0.951      0.995      0.535\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    139/149      1.71G    0.01884     0.0146  0.0006491         59        416: 100% 10/10 [00:01<00:00,  6.63it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.45it/s]\n",
            "                   all          9         18      0.989       0.95      0.995      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    140/149      1.71G    0.01929    0.01538  0.0007388         57        416: 100% 10/10 [00:01<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 10.04it/s]\n",
            "                   all          9         18       0.99      0.948      0.995       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    141/149      1.71G    0.01967    0.01531  0.0009254         73        416: 100% 10/10 [00:02<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.00it/s]\n",
            "                   all          9         18      0.991      0.948      0.995      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    142/149      1.71G    0.02012    0.01432  0.0007979         63        416: 100% 10/10 [00:01<00:00,  5.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 11.67it/s]\n",
            "                   all          9         18      0.995      0.948      0.995      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    143/149      1.71G    0.01925    0.01399  0.0006986         40        416: 100% 10/10 [00:01<00:00,  6.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 14.05it/s]\n",
            "                   all          9         18      0.991      0.948      0.995      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    144/149      1.71G    0.01997    0.01443  0.0005187         81        416: 100% 10/10 [00:01<00:00,  6.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.92it/s]\n",
            "                   all          9         18       0.97      0.948      0.995      0.528\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    145/149      1.71G    0.01941    0.01345  0.0005516         41        416: 100% 10/10 [00:01<00:00,  6.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.72it/s]\n",
            "                   all          9         18      0.929      0.975       0.99      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    146/149      1.71G    0.01894    0.01422  0.0007795         50        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.82it/s]\n",
            "                   all          9         18      0.928      0.977       0.99      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    147/149      1.71G    0.01815     0.0134  0.0004792         59        416: 100% 10/10 [00:01<00:00,  6.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.96it/s]\n",
            "                   all          9         18      0.927      0.977       0.99      0.511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    148/149      1.71G    0.01937    0.01433  0.0007282         92        416: 100% 10/10 [00:01<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 13.07it/s]\n",
            "                   all          9         18      0.921      0.985       0.99      0.511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    149/149      1.71G    0.01979    0.01457  0.0007466         38        416: 100% 10/10 [00:01<00:00,  6.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 15.39it/s]\n",
            "                   all          9         18      0.921      0.985       0.99      0.511\n",
            "\n",
            "150 epochs completed in 0.083 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00, 12.28it/s]\n",
            "                   all          9         18      0.973      0.938      0.995      0.628\n",
            "                   fit          9          8          1      0.876      0.995      0.701\n",
            "                others          9         10      0.947          1      0.995      0.555\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch-yolo",
      "language": "python",
      "name": "pytorch-yolo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a5fad42effe4dea8be7f95341e2ea87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc56b855c3b848a99b348b33dde8cca1",
              "IPY_MODEL_6320fef33ae346ba86227989a0296c05",
              "IPY_MODEL_07e6f4e38c844f8695a882bf1ed27f3f"
            ],
            "layout": "IPY_MODEL_6327597360864735b007bbfca2e722e2"
          }
        },
        "bc56b855c3b848a99b348b33dde8cca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76e8d2cf953b4bf6af5a3b92e7e60b91",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_059e298f22fb4d2bbb6898378c21214e",
            "value": "100%"
          }
        },
        "6320fef33ae346ba86227989a0296c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99f4bbb5ccf14623a8a6ed6fc12ee51d",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cd8dee0029a4641a4dbe523af2b0bfc",
            "value": 46830571
          }
        },
        "07e6f4e38c844f8695a882bf1ed27f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de36f4c4878f4081806130f2b7bb5f1f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4a5e3430ff7048caa9358a2920651e2c",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 211MB/s]"
          }
        },
        "6327597360864735b007bbfca2e722e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76e8d2cf953b4bf6af5a3b92e7e60b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "059e298f22fb4d2bbb6898378c21214e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99f4bbb5ccf14623a8a6ed6fc12ee51d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd8dee0029a4641a4dbe523af2b0bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de36f4c4878f4081806130f2b7bb5f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5e3430ff7048caa9358a2920651e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}